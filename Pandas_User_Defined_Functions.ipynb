{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1111ef1-c468-4b5c-af2c-8bbd7afe2945",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pandas user-defined function (UDF): also known as vectorized UDF\n",
    "\n",
    "It is a user-defined function that uses Apache Arrow to transfer data and pandas to work with the data\n",
    "\n",
    "Pandas UDFs allow vectorized operations that can increase performance up to 100x compared to row-at-a-time Python UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0a6af1-9a2b-4fe6-9391-9bc6080aa89c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Type hint in pandas udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66c7efb3-1b73-4a59-b298-72b9f3825e04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- In Python, type hints are used to statically indicate the type of a value. This can be done for variables, parameters, function arguments, and return values\n",
    "- In the context of Pandas UDFs, type hints are used to specify the types of the input and output arguments of the UDF\n",
    "- This can help to improve the the readability, maintainability of the code, and helps to catch errors at compile time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe29f99-5ba7-40a9-969e-aa8ff5572aeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f437d5ce-83ff-4b4f-aa15-8fe7826dfa09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def multiply(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec15f78-1f16-44c8-bf2b-b27928ed450b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = pd.Series([2, 3, 4])\n",
    "y = pd.Series([5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ca71a0-fb99-4274-81c1-46028b7c0ea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10\n1    18\n2    28\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(multiply(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1551dc3-9aeb-4f50-81fa-ce831575f1e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Series to Series UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f624a4b6-c061-4422-a4ae-264e87ff97ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Use: To vectorize scalar operations & with APIs such as select and withColumn\n",
    "- Spark runs a pandas UDF by splitting columns into batches, calling the function for each batch as a subset of the data, then concatenating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf180ee3-c727-4565-bd09-bc42cf2be726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63541383-8131-4d5b-890c-87fe0cdaef51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    " \n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a18307-f684-4ee0-af11-fc666570be85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n1    4\n2    9\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test the Function Locally\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b70ab803-d116-4443-9a19-424a8db7796f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573f7459-1a71-45c3-8559-0b94ab469235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|  x|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79fa002-1f03-4eba-b47e-c03cec0993b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|multiply_func(x, x)|\n+-------------------+\n|                  1|\n|                  4|\n|                  9|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e73bda8-49ea-4082-803e-abd45b3e4c55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Iterator of Series to Iterator of Series UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be50b33-67a9-497b-a246-b2f0afa8c469",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "An iterator UDF is the same as a scalar pandas UDF except:\n",
    "\n",
    "1. The Python function: Takes an iterator of batches instead of a single input batch as input, Returns an iterator of output batches instead of a single output batch\n",
    "2. The length of the entire output in the iterator should be the same as the length of the entire input\n",
    "3. The wrapped pandas UDF takes a single Spark column as an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24642bb-ed62-49b7-ae4b-41d351c1d906",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Useful when the UDF execution requires initializing some state\n",
    "- For example, loading a ML model file to apply inference to every input batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a96fa39-7f23-4d1e-ac51-3c82c9076277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col, pandas_udf, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c616497-1045-44cc-af30-ae9242ebb6bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame\n",
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    " \n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15913d92-acd9-4dd2-b4cc-48b35e49cf09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|  x|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521a1bb7-33b6-44c9-8ead-3442ae63e59e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a Pandas UDF 'plus_one'\n",
    " \n",
    "# When the UDF is called with the column, the input to the underlying function is an iterator of pd.Series\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in batch_iter:\n",
    "        yield x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eeae601-549c-462c-94e9-a1b7e9c98d97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|plus_one(x)|\n+-----------+\n|          2|\n|          3|\n|          4|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the 'plus_one' UDF to the \"x\" column of the Spark DataFrame\n",
    "df.select(plus_one(col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792edf4f-affe-4b12-973a-5faf4da3d1eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define another Pandas UDF named 'plus_y'\n",
    " \n",
    "y_bc = spark.sparkContext.broadcast(1)\n",
    " \n",
    "@pandas_udf(\"long\")\n",
    "def plus_y(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    y = y_bc.value  # initialize states\n",
    "    try:\n",
    "        for x in batch_iter:\n",
    "            yield x + y\n",
    "    finally:\n",
    "        pass  # release resources here, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97e3bf1-cc39-4587-a080-a7201b27c35d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|plus_y(x)|\n+---------+\n|        2|\n|        3|\n|        4|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the 'plus_y' UDF to the \"x\" column of the Spark DataFrame\n",
    "df.select(plus_y(col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7fff3bd-3f32-4e92-a0dc-4ee209409162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Iterator of multiple Series to Iterator of Series UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2dde10-bb33-4d78-9c9a-29f5c25f047f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- Similar characteristics and restrictions as Iterator of Series to Iterator of Series UDF\n",
    "- The specified function takes an iterator of batches and outputs an iterator of batches\n",
    "- It is also useful when the UDF execution requires initializing some state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d723c072-5ca7-435e-93fa-969acd3e2a4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The differences are:\n",
    "\n",
    "- The underlying Python function takes an iterator of a tuple of pandas Series\n",
    "- The wrapped pandas UDF takes multiple Spark columns as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec3348d-af39-4353-9bb4-7db13e50e0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from typing import Iterator, Tuple\n",
    "from pyspark.sql.functions import col, pandas_udf, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78ef5c0-e4d3-44e4-889a-b490e5e4f21a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a Pandas DataFrame\n",
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    " \n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8ef9d3-6dd4-47bd-8f51-ba1625214084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|  x|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8c2be93-95e7-40a2-b586-675e08f02ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pandas_User_Defined_Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
